{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c14f8e4",
   "metadata": {},
   "source": [
    "# Stock fetching, Cleaning, and Storage\n",
    "#### This notebook is going to:\n",
    "\n",
    "###### Fetch 3 non consecutive 2 month slices of Stock data for 2 companies from the year 2024, this example will use APPLE and Amazon\n",
    "###### Each of these months will be proccessed differently\n",
    "###### The first month will be written directly to a SQL table\n",
    "###### The second month will be written to a JSON file\n",
    "###### The third month will be written to a CSV file\n",
    "\n",
    "###### We will then load and randomly scramble each of the APPLE files before appending them the SQL table\n",
    "\n",
    "###### Finally we are going to build a dataframe from the SQL table, determine the missing months, make the appropriate API calls to retrieve the missing months, complete the data frame and update the SQL table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c780e",
   "metadata": {},
   "source": [
    "### Imports & Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# Import handlers\n",
    "from scripts.handlers.stockHandler import AVStockDataHandler\n",
    "from scripts.handlers.storageHandler import storageHandler\n",
    "from scripts.handlers.helperHandler import helperHandler\n",
    "from scripts.handlers.scrambleHandler import scrambleHandler\n",
    "from scripts.handlers.SQLHandler import SQLHandler\n",
    "from scripts.handlers.cleaningHandler import cleaningHandler\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../config.env')\n",
    "\n",
    "# Retrieve the API key and DB_PATH from the environment variables\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "DB_PATH = os.getenv(\"DB_PATH\")\n",
    "\n",
    "# Raise an error if the API key or DB_PATH is not found\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not found. Please set the ALPHA_VANTAGE_API_KEY in config.env.\")\n",
    "\n",
    "if not DB_PATH:\n",
    "    raise ValueError(\"Database path not found. Please set the DB_PATH in config.env.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2997838",
   "metadata": {},
   "source": [
    "### Handlers, Tickers, And Date Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the handlers\n",
    "stock_handler = AVStockDataHandler(API_KEY)\n",
    "storage_handler = storageHandler()\n",
    "SQL_handler = SQLHandler(DB_PATH)\n",
    "scramble_handler = scrambleHandler()\n",
    "helper_handler = helperHandler()\n",
    "cleaning_handler = cleaningHandler()\n",
    "\n",
    "# Tickers to fetch data for\n",
    "tickers = ['AAPL', 'AMZN']\n",
    "\n",
    "# Date range for fetching data\n",
    "date_ranges = [\n",
    "    ('2024-01-01', '2024-02-29'),\n",
    "    ('2024-05-01', '2024-06-30'),\n",
    "    ('2024-09-01', '2024-10-31')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a031e",
   "metadata": {},
   "source": [
    "### Fetching and Storing ticker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a671aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetch the data, \n",
    "sliced_ticker_data = stock_handler.fetch_multiple_tickers(tickers, date_ranges)\n",
    "\n",
    "# Using heloperHandler to slice the data and create sets\n",
    "sets = helper_handler.create_sets(sliced_ticker_data, date_ranges)\n",
    "\n",
    "SQL_handler.save_dfs_to_table(sets[0])\n",
    "storage_handler.multiple_dfs_to_csv_and_json(sets[1], file_type='csv')\n",
    "storage_handler.multiple_dfs_to_csv_and_json(sets[2], file_type='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e4040",
   "metadata": {},
   "source": [
    "### File Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex patterns for locating AAPL csv and json files\n",
    "AAPL_pattern = r\".*AAPL.*\"  # Matches any CSV file starting with \"AAPL\"\n",
    "\n",
    "# Use the helper handler to locate files matching the patterns\n",
    "aapl_files = helper_handler.find_files(\"raw_data\", AAPL_pattern)\n",
    "\n",
    "# Build dataframes from the located files\n",
    "aapl_dfs = []\n",
    "for file in aapl_files:\n",
    "    df = storage_handler.df_builder(file)\n",
    "    print(dt.head())\n",
    "    aapl_dfs.append(df)\n",
    "# Concatenate the dataframes into a single dataframe\n",
    "aapl_df = pd.concat(aapl_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc407ca0",
   "metadata": {},
   "source": [
    "### Scrambling data\n",
    "\n",
    "###### NOTE: This scramble method can return a new object but the operations are performed in place on the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the concatenated dataframe\n",
    "print(aapl_df)\n",
    "\n",
    "# Scramble the data\n",
    "scramble_handler.scramble_df(aapl_df)\n",
    "\n",
    "# Print the scrambled dataframe\n",
    "print(aapl_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
